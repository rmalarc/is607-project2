---
title: "IS607 - Project 2"
author: "Mauricio Alarcon, Jamey Etherton"
date: "March 14, 2015"
output: html_document
---

* * *

##<a name="toc"></a>Table of Contents
* [Introduction](#introduction)
* [Methodology](#methodology)
* Exploratory Analysis
    * [Descriptive Statistics](#descriptive_statistics)
    * [y=f(x)](#y_fx)
        * [Group I](#group_i)
        * [Group II](#group_ii)
        * [Group III](#group_iii)
        * [Group IV](#group_iv)
* [Outlier Handling](#outlier)
* [Data Modeling](#data_modeling)
    * [Group I](#group_i_m)
    * [Group II](#group_ii_m)
    * [Group III](#group_iii_m)
    * [Group IV](#group_iv_m)
* [Conclusion](#conclusion)

* * *

```{r, echo=FALSE,results = 'hide', warning=FALSE, message=FALSE}
library("ggplot2")
library("boot")
library("pastecs")
library("reshape")
library("lattice")
library("Hmisc")
library("reshape2")
library("RCurl")

# get files from CSV in github
project2_data_csv <- getURL("https://raw.githubusercontent.com/rmalarc/is607-project2/master/project2_data.csv")
project2_data <- read.csv(text=project2_data_csv,head=TRUE,sep=",",as.is=TRUE)
#project2_data_csv <- "/Users/malarcon/Google Drive/CUNY/IS607/is607-project2/project2_data.csv"
#project2_data <- read.csv(project2_data_csv,head=TRUE,sep=",",as.is=TRUE)

```


##<a name="introduction"></a>Introduction

Project 2 of IS607 class requires to perform an exploratory data analysis of the given four data sets.
The data sets are consisted with two variables (X,Y), and the respective values across four groups (I,II,III,IV).
Our objects in this analysis are;
 - to suggest hypotheses about the cause of observed phenomena,
 - to assess assumptions on which statistical inference will be based,
 - to support the selection of appropriate statistical tools and techniques.
 

[[Back to Top]](#toc)

* * *

##<a name="methodology"></a>Methodology

The methodology of this analysis consists of:

* Capture provided data in CSV format with the following columns: group, x, y
* Load the data into R
* Conduct exploratory analysis including: 
    * Obtain descriptive statistics by group
    * Produce basic y~x charts including linear fitting curve
    * Result interpretation
* Data Modeling

[[Back to Top]](#toc)

* * *
##Exploratory Analysis

###<a name="descriptive_statistics"></a>Descriptive Statistics

#### X Variable
```{r, echo=FALSE}

qplot(group,x, data=project2_data, geom=c("boxplot"),
      fill=group,
      main="Distribution of X Values by Group",
      xlab= "Group",
      ylab="X")+
  scale_fill_discrete(name="Group")+ guides(fill=FALSE)

options(digits=2)
aggregate(project2_data$x,list(Group=project2_data$group),FUN=summary)

# Show X-Variable values and frequency of each
dcast(project2_data, group ~ x, value.var="x",fun.aggregate=length)
```

From the summary and plot above we identified that the values of x in groups I, II and III are identical , with X being an integer uniformly distributed with values spanning from 4 to 14.

Group IV however, has only two numerical values: X=8 (n=10) and X=19 (n=1) having the same mean value as group I,II, and III. 

[[Back to Top]](#toc)

#### Y Variable
```{r, echo=FALSE}

qplot(group,y, data=project2_data, geom=c("boxplot"),
      fill=group,
      main="Distribution of Y Values by Group",
      xlab= "Group",
      ylab="Y")+
  scale_fill_discrete(name="Group")+ guides(fill=FALSE)

aggregate(project2_data$y,list(Group=project2_data$group),FUN=summary)

```

Aside from the fact that the mean of Y across all groups is 7.5, the distribution of its values is different across groups.

[[Back to Top]](#toc)

###<a name="y_fx"></a>y=f(x)

Given the fact that there are differences in the basic distribution of X,Y values, we break the analysis of these combinations for each of the groups (I,II,III,IV).

For this part of the analysis we plot the (x,y) value to formulate the relationship between X and Y pairs alongside a linear approximation.

####<a name="group_i"></a>Group I


```{r, echo=FALSE}
ggplotRegression <- function (fit,color="red") {

require(ggplot2)

ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
  geom_point() +
  stat_smooth(method = "lm", col = color) +
  labs(title = paste("y=",signif(fit$coef[[2]], 2),
                     "x +",signif(fit$coef[[1]],2 ),
                     ", Adj R2=",signif(summary(fit)$adj.r.squared, 5),
                     ", p=",signif(summary(fit)$coef[2,4], 5)))
}


fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="I",])
ggplotRegression(fit1)


```

From the above chart we can see that the data in group I does follow a linear trend, with acceptable R2 and p-values.


[[Back to Top]](#toc)

####<a name="group_ii"></a>Group II

```{r, echo=FALSE}
fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="II",])
ggplotRegression(fit1)
```

Here we see that group II's regression output are identical to those of group I. However, it is evident that a quadratic approximation may be a better fit.

[[Back to Top]](#toc)

####<a name="group_iii"></a>Group III

```{r, echo=FALSE}

fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="III",])
ggplotRegression(fit1)

```

In this case we can see the presence of an outlier (13,12.7). This value clearly skews the curve. If removed, a much better fit can be obtained.

[[Back to Top]](#toc)

####<a name="group_iv"></a>Group IV

```{r, echo=FALSE}

fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="IV",])
ggplotRegression(fit1)


```

This series appears to include multiple Y observations for X=8. The average of these multiple Y values can be used instead.

However, it does not appear that the resulting function would be much different than y=0.5x + 3.

[[Back to Top]](#toc)

* * *
##<a name="outlier"></a>Outlier Handling

For the purpose of fitting the data to a model, we will apply the following manipulations to the data (by group):

* I: Even though the R-squared is relatively low, the fitness to a linear model is not being skewed by outliers but it is intrinsic to the data. Therefore, we will use the data directly without any modifications.
* II: In this case, we will attempt to fit the data to a polynomial model without making any modifications to the data.
* III: There is clearly one outlier which throws the linear model in the wrong direction (x=13,y=12.74). We will remove this data-point in order to obtain a model that more adequately fits the remaining of the data.
* IV: There appear to be multiple Y observations for X=8. We will aggregate these multiple observations into one by averaging all the corresponding Y values. 

[[Back to Top]](#toc)

* * *
##<a name="data_modeling"></a>Data Modeling

####<a name="group_i_m"></a>Group I

```{r, echo=FALSE}

fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="I",])
ggplotRegression(fit1,color="springgreen4")

```

[[Back to Top]](#toc)

####<a name="group_ii_m"></a>Group II

```{r, echo=FALSE}
ggplotRegression_squared <- function (fit,color="red") {

require(ggplot2)

test <- function(x) {fit$coef[[3]]*x^2+fit$coef[[2]]*x+fit$coef[[1]]}
ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
  geom_point() +
  stat_function(fun = test, col = color) +
  labs(title = paste("y=",signif(fit$coef[[3]], 2),"x^2 +"
                     ,signif(fit$coef[[2]], 2),
                     "x +",signif(fit$coef[[1]],2 ),
                     ", Adj R2=",signif(summary(fit)$adj.r.squared, 5),
                     ", p=",signif(summary(fit)$coef[2,4], 5)))
}

fit1 <- lm(y ~ x+ I(x^2), data = project2_data[project2_data$group=="II",])
ggplotRegression_squared(fit1,color="springgreen4")

```

[[Back to Top]](#toc)

####<a name="group_iii_m"></a>Group III

```{r, echo=FALSE}

fit1 <- lm(y ~ x, data = project2_data[project2_data$group=="III"&project2_data$x!=13,])
ggplotRegression(fit1,color="springgreen4")

```

[[Back to Top]](#toc)

####<a name="group_iii_m"></a>Group IV

```{r, echo=FALSE, warning=FALSE}
series4 <- project2_data[project2_data$group=="IV",]
series4<-rbind(data.frame(group="IV"
                          ,x=8
                          ,y=mean(series4[series4$x==8,"y"]))
               ,series4[series4$x!=8,]
               )
fit1 <- lm(y ~ x, data = series4)
ggplotRegression(fit1,color="springgreen4")

```

[[Back to Top]](#toc)

* * *

##<a name="conclusion"></a>Conclusion

[[Back to Top]](#toc)

1.Hypotheses; X variables are predictor and Y variables are response. 

2.Assumption; Based on the linear regression analysis we identified that X and Y move in the same direction without major outliers except
group 4. Under this assumption, we should flag outliers to validate the data quality. One has to be careful when trying to handle outliers in the data.
First, it must be clear what the goal is. As we saw in this project the goal was to obtain a good linear or polynomial fit.
With that goal clear, we retain, removed or aggregated data or adjusted to a more suitable model.

3.Selection of technique; Simple linear regression is suitable to analyze each set of data since there are only two variables 
in each set. However, multiple linear regression with interactions would be another option for the 4 data sets if the goal is to
find out the relationship of the data sets. Our goal was to obtain a good linear regression.
